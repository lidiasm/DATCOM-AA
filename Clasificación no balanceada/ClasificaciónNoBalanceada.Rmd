---
title: "Actividades para Clasificación No Balanceada"
author: "Lidia Sánchez Mérida"
output: pdf_document
---

# Actividad 1

En este primer ejercicio el objetivo consiste en replicar el análisis exploratorio ejemplificado en el guión de prácticas sobre el dataset `subclus`. Antes de comenzar, cargamos las librerías necesarias para tratar con este conjunto de datos.

```{r}
# Cargamos las librerías necesarias para este proyecto
library(caret)
library(imbalance)
library(dplyr)
library(pROC)
library(tidyr)
```

Como podemos apreciar en los siguientes resultados, este dataset cuenta con **599 registros y 3 variables**, dos independientes de tipo entero y una columna categórica compuesta por dos valores nomianles: las clases `negative` y `positive`. 

```{r}
# Cargamos el dataset desde el fichero
subclus.df <- read.csv("subclus.csv", header = TRUE)
# Establecemos los nombres de las columnas
colnames(subclus.df) <- c("Att1", "Att2", "Class")
# Nombramos la clase minoritaria como positiva
subclus.df$Class <- relevel(subclus.df$Class, "positive") 
# Dimensiones del dataset
dim(subclus.df)
# Tipos de datos 
str(subclus.df)
# 5 primeros registros
head(subclus.df)
# Clases 
levels(subclus.df$Class)
```
A continuación podemos apreciar el resumen estadístico del conjunto de datos `subclus` que muestra las medidas estadísticas más comunes para las variables numéricas y el número de ejemplares para cada clase en el caso de las variables categóricas. 

El **atributo `Att1` dispone de una media y mediana muy similares** lo que significa que no existe prácticamente diferencias entre el valor medio y la media de todos sus valores. Si observamos los cuartiles, podemos visualizar que existe una **amplia diversidad de datos** tanto en el primer intervalo compuesto por el mínimo y el primer cuartil, como en el segundo entre el primer cuartil y el segundo cuartil o mediana. No obstante, esta tendencia no ocurre en el tercer intervalo entre la mediana y el tercer cuartil puesto que la diferencia entre sus valores es considerablemente menor que en los casos anteriores.

La segunda variable dependiente `Att2` también se caracteriza por tener una **media y mediana muy parecidas y por una gran diversidad de valores en todos los intervalos** disponibles. Por lo tanto, a diferencia de la variable anterior, este atributo parece no disponer de concentraciones de valores particulares.

Finalmente según el recuento del número de muestras de cada clase, podemos determinar que la variable dependiente `Class` se encuentra **fuertemente desbalanceada** puesto que aproximádamente sólo el 16% de los datos pertenecen a la clase positiva. Esta teoría se encuentra respaldada por el mínimo valor que representa la métrica *imbalance ratio*.

```{r}
# Resumen estadístico del dataset
summary(subclus.df)
# Ratio de balanceado de clases
imbalanceRatio(subclus.df)
```
A continuación se representa un **diagrama de cajas de ambos atributos** para representar gráficamente los datos que contienen cada uno. En primer lugar podemos observar que disponen de **escalas bastante diferentes**, lo cual deberíamos tomarlo en consideración para el entrenamiento de modelos predictivos. Por otro lado, como se destacó en el anterior resumen estadístico, ambas variables disponen de una **gran variabilidad de datos** tal y como se puede apreciar en la longitud de sus respectivas cajas.

```{r}
# Variables independientes (atributos)
x <- subclus.df[,1:2]
# Variable dependiente (clase)
y <- subclus.df[,3]
# Boxplot de los dos atributos
par(mfrow=c(1,2))
for(i in 1:2) {
  boxplot(x[,i], main=names(subclus.df)[i])
}
```
En este segundo gráfico se representa el **desbalanceamiento de las clases** de la variable dependiente mediante los porcentajes de muestras asociados. Como se anticipó anteriormente, el número de ejemplos de la clase positiva apenas alcanza un 16% del total del dataset.

```{r}
# Número de muestras para cada clase
n_classes <- c(sum(y=="positive"), sum(y=="negative"))
# Porcentaje de muestras para cada clases
pct <- round(n_classes/sum(n_classes)*100, digits=2)
# Etiquetas lingüísticas para representar las clases
lbls <- levels(subclus.df$Class)
# Añade los porcentajes de muestras a cada etiqueta
lbls <- paste(lbls, pct) 
lbls <- paste(lbls, "%", sep="")
# Gráfico de tarta para representar el balanceado de clases
pie(n_classes, labels=lbls, main="Class distribution")
```

Continuamos con la representación de gráficos multivariantes como ocurre en el siguiente *chunk*, en el que se muestran los datos asociados a los dos atributos disponibles coloreados según la clase a la que pertenecen. Las matrices de puntos suelen ser útiles para identificar asociaciones entre atributos y con respecto a las propias clases a predecir. Sin embargo en este caso parece que **no existe ningún patrón**.

```{r}
# Matriz de puntos para representar los dos atributos y las muestras según
# a la clase a la que pertenecen
featurePlot(x=x, y=y, plot="ellipse")
```

En el siguiente gráfico se representan los diagramas de cajas relativos a cada uno de los atributos disponibles identificando las muestras pertenecientes a cada clase. Tal y como podemos observar, al igual que en el dataset *circle*, en este conjunto de datos también podemos observar que los **ejemplos positivos se solapan con los negativos** puesto que se encuentran en subintervalos de la clase mayoritaria. Esta característica puede dificultar enormemente el entrenamiento de modelos predictivos puesto que no resulta sencillo diferenciar las muestras que pertenecen a cada clase. 

```{r}
# Diagrama de cajas por atributo y clase
featurePlot(x=x, y=y, plot="box")
```
Para finalizar este primer ejercicio procedemos a entrenar varios modelos predictivos con diversas configuraciones para comparar el comportamiento en cada una de ellas. En particular, vamos a utilizar el algoritmo *K Nearest Neighbors* (KNN) con tres metodologías:

1. Utilizando el **conjunto de datos original sin balancear** las clases.
2. Aplicando **técnicas de *undersampling* y *oversampling* aleatorio** para balancear las clases.
3. Aplicando el **algoritmo *SMOTE* ** para generar muestras sintéticas de la clase minoritaria.

Comenzamos dividiendo el conjunto de datos original en un subconjunto de entrenamiento con un 75% del total y un subconjunto de test con el 25% restante. Como podemos observar en los siguientes resultados ambos conjuntos se siguen caracterizando por un fuerte desbalanceamiento de clases.

```{r}
# Establecemos una semilla para que los resultados sean reproducibles
set.seed(42) 
# Dividimos el dataset en entrenamiento (75%) y test
trainIndex <- createDataPartition(subclus.df$Class, p=0.75, list=FALSE, times=1)
subclus.train <- subclus.df[trainIndex,]
subclus.test <- subclus.df[-trainIndex,]
# Imbalance ratio para ambos datasets
imbalanceRatio(subclus.train)
summary(subclus.train$Class)
imbalanceRatio(subclus.test)
summary(subclus.test$Class)
```
A continuación definimos las dos funciones que nos permitirán entrenar y validar los modelos generados para cada una de las anteriores configuraciones. 

```{r}
# Función para entrenar un modelo con el algoritmo KNN, el dataset y la
# configuración proporcionados
learn_model <- function(dataset, ctrl, message) {
  # Entrenamiento del modelo con centrado y escalado de los datos eligiendo
  # el mejor valor del parámetro K en función de la curva ROC
  model.fit <- train(Class ~ ., data = dataset, method = "knn", 
                     trControl = ctrl, preProcess = c("center","scale"), 
                     metric="ROC", tuneGrid = expand.grid(k = c(1,3,5,7,9,11)))
  # Predicciones sobre el conjunto de entrenamiento
  model.pred <- predict(model.fit, newdata=dataset)
  # Matriz de confusión y curva ROC
  model.cm <- confusionMatrix(model.pred, dataset$Class,positive = "positive")
  model.probs <- predict(model.fit,newdata = dataset, type="prob")
  model.roc <- roc(dataset$Class,model.probs[,"positive"],color="green")
  return(model.fit)
}

# Función para evaluar un modelo previamente entrenado sobre el conjunto de
# datos proporcionado
test_model <- function(dataset, model.fit, message) {
  # Predicciones sobre el conjunto de test
  model.pred <- predict(model.fit, newdata = dataset)
  # Matriz de confusión y curva ROC
  model.cm <- confusionMatrix(model.pred, dataset$Class, positive = "positive")
  print(model.cm)
  model.probs <- predict(model.fit, newdata = dataset, type="prob")
  model.roc <- roc(dataset$Class, model.probs[,"positive"])
  plot(model.roc, type="S", print.thres= 0.5, main=c("ROC Test", message), col="blue")
  return(model.cm)
}
```

En este primer modelo se han utilizado las dos particiones anteriores para entrenamiento y test **sin ningún tipo de preprocesamiento**. Como podemos observar en el primer gráfico, parece que el **número óptimo de vecinos** a considerar en el algoritmo KNN se encuentra **entre 4 y 6** en caso de que queramos maximizar el área bajo la curva ROC. Por otro lado, si observamos las métricas de calidad del modelo podemos apreciar que ha conseguido una tasa de acierto del 89.93%. Sin embargo esta medida no es representativa puesto que el **clasificador está fuertemente sesgado por la clase mayoritaria**. Este hecho provoca que la métrica *sensitivity* no alcance el 60% de muestras positivas bien clasificadas puesto que la respuesta por defecto de este modelo es la clase negativa. Adicionalmente, el tercer gráfico muestra que el clasificador consigue un **área bajo la curva ROC del 58.3%**, lo que significa que no dispone de una buena capacidad de generalización.

```{r}
# Configuración de entrenamiento
## Validación cruzada con 5 particiones durante 3 iteraciones
ctrl <- trainControl(method="repeatedcv", number=5, repeats=3, 
                     classProbs=TRUE, summaryFunction=twoClassSummary)
# Entrenamos el modelo con los datos sin balancear
model.raw <- learn_model(subclus.train, ctrl, "RAW")
# Graficamos los resultados del modelo
plot(model.raw,main="Grid Search RAW")
print(model.raw)
cm.raw <- test_model(subclus.test, model.raw, "RAW")
```

A continuación **aplicamos *undersampling* aleatorio** para reducir el número de muestras de la clase negativa al número de ejemplares de la clase minoritaria. Como podemos observar en los siguientes resultados la **tasa de aciertos disminuye hasta un 84.56%**, equilibrando los valores de *sensitivity* y *specificity*. Adicionalmente, como consecuencia podemos observar un **considerable aumento del área bajo la curva ROC** hasta situarse en un 87.5%, lo cual nos indica que balancear las clases de la variable a predecir mediante *undersampling* aleatorio incrementa la capacidad de generalización del clasificador.

```{r}
# Configuración de entrenamiento
## Validación cruzada con 5 particiones durante 3 iteraciones
## Undersampling sobre la clase mayoritaria
ctrl <- trainControl(method="repeatedcv", number=5, repeats = 3, 
                     classProbs=TRUE, summaryFunction = twoClassSummary, 
                     sampling = "down")
# Entrenamos el modelo aplicando undersampling aleatorio
model.us <- learn_model(subclus.train, ctrl, "US")
# Evaluamos el modelo y representamos las métricas de calidad
cm.us <- test_model(subclus.test, model.us, "US")
```
La técnica contraria a la anterior es **el *oversampling* aleatorio** que consiste en replicar muestras de la clase minoritaria hasta alcanzar el mismo número de ejemplares de la clase negativa. Con esta configuración la **tasa de aciertos para el tercer clasificador disminuye** aún más hasta situarse en un 81.88%. Parece lógico pensar que a mayor número de datos, mayor es la probabilidad de error al clasificar una muestra. Un aspecto destacable es su **100% de acierto al detectar las muestras negativas**. Este hecho solo se ha logrado con esta configuración al balancear el número de ejemplares de cada clase sin reducir la representación de la mayoritaria.

```{r}
# Configuración de entrenamiento
## Validación cruzada con 5 particiones durante 3 iteraciones
## Oversampling sobre la clase minoritaria
ctrl <- trainControl(method="repeatedcv", number=5, repeats=3, classProbs=TRUE, 
                     summaryFunction=twoClassSummary, sampling="up")
# Entrenamos el modelo aplicando oversampling aleatorio
model.os <- learn_model(subclus.train, ctrl, "OS")
# Evaluamos el modelo y representamos las métricas de calidad
cm.os <- test_model(subclus.test, model.os, "OS")
```
Finalmente aplicamos **SMOTE** para balancear las clases generando muestras sintéticas con este algoritmo más sofisticado. Mediante este conjunto de entrenamiento se entrena un **cuarto modelo predictivo cuya tasa de acierto es de 85.91%**, la más alta conseguida hasta el momento. Este hecho nos indica que utilizar esta técnica para equilibrar el número de ejemplares de cada clase ha ayudado al clasificador a **distinguir mejor los dos tipos de clases** disponibles. De ese modo se disponen de valores considerablemente altos tanto para las métricas *sensivitiy* y *specificity*, como para el área bajo la curva ROC que se sitúa en un 91.7%.  

```{r}
# Configuración de entrenamiento
## Validación cruzada con 5 particiones durante 3 iteraciones
## Oversampling sobre la clase minoritaria
ctrl <- trainControl(method="repeatedcv", number=5, repeats=3, classProbs=TRUE,
                     summaryFunction=twoClassSummary, sampling="smote")
# Entrenamos el modelo aplicando oversampling aleatorio
model.smt <- learn_model(subclus.train, ctrl, "SMT")
# Evaluamos el modelo y representamos las métricas de calidad
cm.smt <- test_model(subclus.test, model.smt, "SMT")
```

Tras generar varios con diferentes configuraciones y métodos de preprocesamiento, a continuación se muestran diversos resúmenes de algunas de las métricas de calidad más relevantes para elegir el más adecuado a este problema. Según el **área bajo la curva ROC** el modelo que consigue el valor más alto es aquel en el que se ha usado SMOTE para balancear las clases de la variable a predecir. No obstante, observando la **métrica sensitivity** el clasificador que mejor predice la clase positiva es el relativo a la aplicación de *oversampling* para mejorar la representación de esta categoría. Sin embargo, no proporciona tan buenos resultados considerando únicamente la **medida specificity** puesto que en este caso es el modelo con los datos en bruto el que mejor consigue clasificar la etiqueta negativa que se corresponde con la clase mayoritaria. Si tuviésemos que decidirnos por un modelo en base a unas métrica de cierta calidad y con la capacidad de predecir la clase minoritaria, podríamos optar por el **tercer clasificador con oversampling** que ronda el 90% de curva ROC y maximiza los valores de la medida *sensitivity*.

```{r}
# Resumen tabular de los modelos
models <- list(raw = model.raw,us = model.us,os = model.os,smt = model.smt)
results <- resamples(models)
summary(results)
```

A continuación se representa gráficamente el resumen mostrado anteriormente. En este tipo de visualizaciones es más sencillo analizar el comportamiento de los diferentes algoritmos de preprocesamiento en cada uno de los modelos entrenados. En el caso de **SMOTE podemos observar que existe una gran variabilidad de resultados para la métrica *sensitivity* **. Su rendimiento se puede ver afectado dependiendo de la calidad de las muestras positivas que haya generado para cada partición de la validación cruzada. Por otro lado, podemos apreciar que aplicar **oversampling y undersampling son beneficiosos para *sensitivity* ** puesto que sus resultados son muy similares. Y es que mientras que con la primera técnica se replican aquellos ejemplos más relevantes ganando información para el modelo, con la segunda se pueden eliminar instancias ruidosas que ayuden a clarificar las fronteras entre sendas clases.

```{r}
# Representación del resumen estadístico anterior de los cuatro modelos
bwplot(results)
```

En este último gráfico se encuentran representadas las medidas anteriores y una nueva denominada *F1* que demuestra el equilibrio entre la precisión y el número de muestras positivas bien clasificadas. Mientras que en las restantes métrica los modelos presentan un mayor número de diferencias, en esta medida podemos observar que se encuentran en valores muy similares siendo SMOTE el que dispone del mayor valor. Esta teoría indica que el algoritmo de preprocesamiento **SMOTE consigue un buen porcentaje de acierto a la vez que clasifica correctamente el mayor número de muestras de la clase positiva**. 

```{r}
# Comparativa de medidas de calidad para todos los modelos
comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))
# Representación gráfica de la comparativa
for (name in names(models)) {
  cm_model <- get(paste0("cm.", name))
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    mutate(Sensitivity = cm_model$byClass["Sensitivity"],
           Specificity = cm_model$byClass["Specificity"],
           Precision = cm_model$byClass["Precision"],
           F1 = cm_model$byClass["F1"])
}
comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 3)
```


# Actividad 2

En este segundo ejercicio se pretende aplicar diferentes algoritmos de la familia SMOTE para conocer qué rasgos característicos presentan sus comportamientos frente a diversos conjuntos de datos. En mi caso se han seleccionado los cuatro siguientes:

* **SMOTE**. Se trata de algoritmo original que consigue aumentar el número de muestras de la clase minoritaria a través de la interpolación de ejemplares que componen un vecindario con K participantes.

* **ANSMOTE**. El algoritmo *Adaptive Neighbor SMOTE* es una variante de la técnica anterior en la que se **calcula el número de vecinos** a considerar para cada una de las muestras de la clase minoritaria automáticamente.

* **DBSMOTE**. Esta tercera variante conocida como *Density-based SMOTE* utiliza **técnicas de clustering** para agrupar las muestras de la clase minoritaria con las que generar los ejemplos sintéticos a partir de las distancias entre las originales. 

* **SLMOTE**. El último algoritmo de la familia SMOTE es capaz de definir una especie de *área segura* en la que situar las muestras sintéticas de la clase minoritaria de modo que no se encuentren cerca de ejemplares de la etiqueta mayoritaria y así no provocar errores de clasificación.

```{r}
# Cargamos los cuatro datasets con los que se van a realizar los experimentos
data("ecoli1")
data("glass0")
data("haberman")
data("yeast4")
# Generamos los diferentes conjuntos de datos con los diversos algoritmos SMOTE
# y entrenamos un modelo por cada uno de ellos

# Entrenamiento de modelos
# ctrl <- trainControl(method="repeatedcv", number=5, repeats=3, classProbs=TRUE,
#                      summaryFunction=twoClassSummary, sampling="smote")
# model.smt <- learn_model(subclus.train, ctrl, "SMT")
# cm.smt <- test_model(subclus.test, model.smt, "SMT")
```

